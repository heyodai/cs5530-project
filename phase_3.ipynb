{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 3 - Model Training and Evaluation\n",
    "\n",
    "Goals for this phase:\n",
    "\n",
    "1. Split the dataset into training, validation, and test sets.\n",
    "2. Train the model on the training set and monitor its performance on the validation set. \n",
    "3. Evaluate the model on the test set to get a final estimate of its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import (DistilBertTokenizer, DistilBertModel,\n",
    "                          BertTokenizer, BertModel,\n",
    "                          AlbertTokenizer, AlbertModel,\n",
    "                          ElectraTokenizer, ElectraModel,\n",
    "                          MobileBertTokenizer, MobileBertModel)\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pyemd import emd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA is available\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Check if MPS is available\n",
    "use_mps = False\n",
    "if use_cuda:\n",
    "    try:\n",
    "        torch.cuda.amp.autocast()\n",
    "        use_mps = True\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Raise an error if neither CUDA nor MPS is available\n",
    "if not use_cuda and not use_mps:\n",
    "    raise RuntimeError(\"CUDA or MPS is required for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "data = pd.read_csv(\"cleaned.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop rows with no summary\n",
    "# data.dropna(subset=['summary'], inplace=True)\n",
    "\n",
    "# # Define stop words and lemmatizer\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# # Tokenize and lemmatize the content\n",
    "# data['tokenized_content'] = data['content'].apply(lambda x: [lemmatizer.lemmatize(\n",
    "#     word) for word in word_tokenize(x.lower()) if word.isalpha() and word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the models\n",
    "models = {\n",
    "    'distilbert-base-uncased': (DistilBertTokenizer, DistilBertModel),\n",
    "    'bert-base-uncased': (BertTokenizer, BertModel),\n",
    "    'albert-base-v2': (AlbertTokenizer, AlbertModel),\n",
    "    'google/electra-small-discriminator': (ElectraTokenizer, ElectraModel),\n",
    "    'google/mobilebert-uncased': (MobileBertTokenizer, MobileBertModel)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the similarity measures\n",
    "def smooth_inverse_frequency(word, doc_freqs, num_docs):\n",
    "    idf = np.log((num_docs + 1) / (doc_freqs[word] + 1))\n",
    "    return idf\n",
    "\n",
    "\n",
    "def word_movers_distance(model, tokenizer, doc1, doc2):\n",
    "    doc1_tokens = tokenizer.tokenize(doc1)\n",
    "    doc2_tokens = tokenizer.tokenize(doc2)\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "        doc1_embedding = model(\n",
    "            doc1, return_tensors=\"pt\").last_hidden_state.squeeze().detach().cpu().numpy()\n",
    "        doc2_embedding = model(\n",
    "            doc2, return_tensors=\"pt\").last_hidden_state.squeeze().detach().cpu().numpy()\n",
    "    else:\n",
    "        doc1_embedding = model(\n",
    "            doc1, return_tensors=\"pt\").last_hidden_state.squeeze().detach().numpy()\n",
    "        doc2_embedding = model(\n",
    "            doc2, return_tensors=\"pt\").last_hidden_state.squeeze().detach().numpy()\n",
    "    distance_matrix = cosine_similarity(doc1_embedding, doc2_embedding)\n",
    "    distance_matrix /= distance_matrix.max()\n",
    "    word2id = tokenizer.get_vocab()\n",
    "    id2word = {v: k for k, v in word2id.items()}\n",
    "    doc1_counts = [doc1_tokens.count(id2word[i]) for i in range(len(word2id))]\n",
    "    doc2_counts = [doc2_tokens.count(id2word[i]) for i in range(len(word2id))]\n",
    "    doc_freqs = np.array([sum([1 for d in data['tokenized_content']\n",
    "                               if id2word[i] in d]) for i in range(len(word2id))])\n",
    "    num_docs = len(data['tokenized_content'])\n",
    "    doc1_sif = np.array([smooth_inverse_frequency(id2word[i], doc_freqs, num_docs) * doc1_counts[i]\n",
    "                            for i in range(len(word2id))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the models\n",
    "for model_name, (tokenizer_class, model_class) in models.items():\n",
    "    print(f\"Training {model_name}...\")\n",
    "    \n",
    "    tokenizer = tokenizer_class.from_pretrained(model_name)\n",
    "    if use_mps:\n",
    "        model = model_class.from_pretrained(model_name).to(\n",
    "            device=torch.device('cuda'),\n",
    "            non_blocking=True).to(memory_format=torch.channels_last)\n",
    "        scaler = torch.cuda.amp.GradScaler()\n",
    "    else:\n",
    "        model = model_class.from_pretrained(model_name).to(\n",
    "            device=torch.device('cuda'))\n",
    "        \n",
    "    num_docs = len(data)\n",
    "    doc_freqs = np.zeros(len(tokenizer))\n",
    "\n",
    "    for doc in data['tokenized_content']:\n",
    "        for word in set(doc):\n",
    "            doc_freqs[tokenizer.convert_tokens_to_ids(word)] += 1\n",
    "\n",
    "    doc_freqs = np.where(doc_freqs > 0, doc_freqs, 1)\n",
    "\n",
    "    for i, row in data.iterrows():\n",
    "        summary = row['summary']\n",
    "        content = row['content']\n",
    "        tokenized_content = row['tokenized_content']\n",
    "\n",
    "        # Compute the TF-IDF weights for the tokenized content\n",
    "        vectorizer = TfidfVectorizer(tokenizer=lambda x: x, lowercase=False)\n",
    "        tfidf_weights = vectorizer.fit_transform(tokenized_content)\n",
    "        tfidf_weights = tfidf_weights.toarray()\n",
    "\n",
    "        # Compute the weighted average embedding for the tokenized content\n",
    "        content_embedding = np.average(\n",
    "            tfidf_weights[:, :, np.newaxis] * model(tokenized_content,\n",
    "                                                    return_tensors=\"pt\").last_hidden_state.squeeze().detach().cpu().numpy(),\n",
    "            axis=1)\n",
    "        \n",
    "        # Compute the Word Mover's Distance and Smooth Inverse Frequency scores for the summary and each document\n",
    "        wmd_scores = []\n",
    "        sif_scores = []\n",
    "        for j, other_row in data.iterrows():\n",
    "            if i != j:\n",
    "                other_content = other_row['content']\n",
    "                other_tokenized_content = other_row['tokenized_content']\n",
    "                other_summary = other_row['summary']\n",
    "                other_tfidf_weights = vectorizer.transform(other_tokenized_content).toarray()\n",
    "                other_content_embedding = np.average(\n",
    "                    other_tfidf_weights[:, :, np.newaxis] * model(other_tokenized_content,\n",
    "                                                                   return_tensors=\"pt\").last_hidden_state.squeeze().detach().cpu().numpy(),\n",
    "                    axis=1)\n",
    "                wmd_score = word_movers_distance(model, tokenizer, summary, other_summary)\n",
    "                sif_score = cosine_similarity(content_embedding.reshape(1, -1),\n",
    "                                               other_content_embedding.reshape(1, -1),\n",
    "                                               smooth_inverse_frequency, doc_freqs, num_docs)\n",
    "                wmd_scores.append(wmd_score)\n",
    "                sif_scores.append(sif_score)\n",
    "\n",
    "        # Compute the average Word Mover's Distance and Smooth Inverse Frequency scores for the summary\n",
    "        avg_wmd_score = np.mean(wmd_scores)\n",
    "        avg_sif_score = np.mean(sif_scores)\n",
    "\n",
    "        # Print the results\n",
    "        print(f\"Model: {model_name}\")\n",
    "        print(f\"Article ID: {row['id']}\")\n",
    "        print(f\"Word Mover's Distance: {avg_wmd_score}\")\n",
    "        print(f\"Smooth Inverse Frequency: {avg_sif_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
