{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Model Architecture Selection\n",
    "\n",
    "We have two goals for this phase:\n",
    "\n",
    "1. Choose a deep learning model that is suitable for the task of summarization.  \n",
    "2. Define the architecture of the model, including the number of layers, hidden units, and embedding dimensions.\n",
    "\n",
    "Note that this notebook does not have any code. It is just a place to record our thoughts and ideas. We will do a code-based investigation during Phase 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Choose a Model\n",
    "\n",
    "We will most likely go with a transformer-based model, such as BERT, T5, GPT-J, or GPT-Neo. However, we could also look into an encoder-decoder model or a pointer-generator network. It may be best to analyze the pros and cons of each model and run a few experiments to see which one works best for our task.\n",
    "\n",
    "Here's a table of the models that we are considering:\n",
    "\n",
    "| Model                                                      | Type                    | Key Features                                                                                                              |\n",
    "|------------------------------------------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------|\n",
    "| BERT                                                       | Transformer-based model | Pre-trained, fine-tunable, attention-based, bidirectional                                                                 |\n",
    "| T5                                                         | Transformer-based model | Pre-trained, fine-tunable, text-to-text transfer learning, flexible architecture                                          |\n",
    "| GPT-J                                                      | Transformer-based model | Pre-trained, large scale, high-quality generation                                                                         |\n",
    "| GPT-Neo                                                    | Transformer-based model | Pre-trained, smaller than GPT-J, high-quality generation                                                                  |\n",
    "| Sequence-to-Sequence with Attention (Seq2Seq)              | Encoder-Decoder model   | Attention-based, flexible architecture, proven success for text summarization                                             |\n",
    "| Transformer-Based Encoder-Decoder Models (BART, MASS, XLM) | Encoder-Decoder model   | Transformer-based, pre-trained, fine-tunable, bidirectional, flexible architecture, proven success for text summarization |\n",
    "| Pointer-Generator Network for Abstractive Summarization    | Pointer-Generator model | Hybrid approach (extraction and abstraction), proven success for text summarization                                       |\n",
    "| Reinforcement Learning-Based Pointer-Generator Network     | Pointer-Generator model | Trained using reinforcement learning, state-of-the-art performance, proven success for text summarization                 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update (Apr 15, 2023)\n",
    "\n",
    "The models we discuss above are generally far too large for us to train in a reasonable timeframe (for a class project, with access to 2 GPUs). As such, we are going to focus on the following models which are faster to train:\n",
    "\n",
    "1. **DistilBERT:** DistilBERT is a smaller and faster version of the popular BERT model, with fewer parameters and a faster training time. Despite its smaller size, it has been shown to achieve similar performance to BERT on a variety of natural language processing tasks.\n",
    "2. **MobileBERT:** MobileBERT is another smaller and faster version of BERT that is designed to be more memory-efficient, making it well-suited for deployment on mobile devices or other resource-constrained environments.\n",
    "3. **ALBERT:** ALBERT (A Lite BERT) is another smaller and faster version of BERT that is designed to be more parameter-efficient by using a factorized embedding parameterization and cross-layer parameter sharing.\n",
    "4. **TinyBERT:** TinyBERT is a smaller and faster version of BERT that is trained using a distillation-based approach, where a larger teacher model is used to guide the training of the smaller student model.\n",
    "5. **ELECTRA:** ELECTRA is a smaller and faster alternative to BERT that is trained using a novel pretraining objective based on a generator-discriminator architecture, which has been shown to achieve similar performance to BERT on a variety of tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: Define the Architecture\n",
    "\n",
    "We will need to decide on the number of layers, hidden units, and embedding dimensions for our model. The number of layers and hidden units will depend on the size of our dataset and the level of performance we want to achieve. The embedding dimensions will depend on the size of our vocabulary and the level of performance we want to achieve.\n",
    "\n",
    "The ideal numbers will vary depending on the model we choose. For example, a transformer-based model like BERT will have a different architecture than an encoder-decoder model like Seq2Seq. However, here are some general guidelines:\n",
    "\n",
    "1. **Number of Layers** \n",
    "    - For transformer-based models like BERT and GPT, a typical architecture might include 12-24 layers. \n",
    "    - For encoder-decoder models like Seq2Seq, a typical architecture might include 2-4 layers.\n",
    "2. **Hidden Units** \n",
    "    - For transformer-based models, the number of hidden units per layer can range from 64 to several thousand. \n",
    "    - For encoder-decoder models, a typical architecture might include 128-512 hidden units per layer.\n",
    "3. **Embedding Dimensions** \n",
    "    - For transformer-based models, the embedding dimensions can range from 128 to several thousand. \n",
    "    - For encoder-decoder models, a typical architecture might include 128-512 embedding dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4286f1ee66e41fbf26cca8876906444877a06270d30416029ae0afaadf35139a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
