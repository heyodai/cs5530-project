{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 - Data Acquisition and Refinement\n",
    "\n",
    "The goals for Phase 1, per the roadmap in the readme, are as follows:\n",
    "\n",
    "1. Obtain a dataset of news articles that includes the text content as well as a summary of each article. \n",
    "2. Explore the dataset to get a sense of the data, such as the number of articles, length of the articles and summaries, and distribution of topics and keywords.\n",
    "3. Clean and preprocess the data to remove unnecessary characters, punctuation, and stop words. \n",
    "4. Tokenize the text into words or subwords, and create input sequences and output summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Obtain a Dataset\n",
    "\n",
    "We've obtained a dataset from Kaggle that contains 870,521 articles, each of which has a text content and a summary. The text content is the full article, and the summary is a short version of the article that summarizes the main points.\n",
    "\n",
    "For more information on the dataset, please see the dataset section in `README.md`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import WhitespaceTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/raw.csv') # expect this step to take about 30 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: Explore the Dataset\n",
    "\n",
    "We've explored the dataset and found the following information:\n",
    "- Total of 870,521 articles\n",
    "- Only 580,013 unique articles (about 1/3 of the articles are duplicates)\n",
    "- The most frequently occurring words are generic words like \"this\", \"that\", \"the\", \"a\", \"an\", etc.\n",
    "  - Should we remove these words from the dataset?\n",
    "\n",
    "The dataset contains the following columns:\n",
    "- `Unnamed: 0` - index, can be ignored\n",
    "- `ID` - unique ID for each article (appears to be a generated UUID)\n",
    "- `Content` - the text content of the article\n",
    "- `Summary` - the summary of the article\n",
    "- `Dataset` - the dataset that the article came from (XSum, CNN/Daily Mail, Multi-News)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(df.columns[0], axis=1) # unused index column\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find the average of the Content and Summary columns\n",
    "# \n",
    "# We can't analyze the entire dataset because it's too large, so we'll take a sample of 1000 rows.\n",
    "df_sample = df.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    df_sample['Content'].apply(len),\n",
    "    df_sample['Summary'].apply(len),\n",
    "    df_sample['Content'].apply(lambda x: len(x.split())),\n",
    "    df_sample['Summary'].apply(lambda x: len(x.split())),\n",
    "    df_sample['Content'].apply(lambda x: len(x.split('.'))),\n",
    "    df_sample['Summary'].apply(lambda x: len(x.split('.')))\n",
    "]\n",
    "\n",
    "sample = pd.DataFrame(data, index=['Content Length', 'Summary Length', 'Content Words', 'Summary Words', 'Content Sentences', 'Summary Sentences']).T.describe().round(2)\n",
    "\n",
    "sample.loc['mean']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's look at distribution of topics and keywords.\n",
    "#\n",
    "# The dataset doesn't provide topics, so we'll parse keywords from the summary column.\n",
    "df_sample2 = df.sample(10000)\n",
    "df_sample2['Keywords'] = df_sample2['Summary'].apply(lambda x: x.split()[-5:])\n",
    "\n",
    "# Now let's list all the keywords and their frequencies.\n",
    "keywords = []\n",
    "for i in df_sample2['Keywords']:\n",
    "    keywords.extend(i)\n",
    "\n",
    "keywords = pd.Series(keywords).value_counts()\n",
    "\n",
    "# Let's plot the top 20 keywords as a word cloud.\n",
    "# keywords = keywords[20:]\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=20, background_color='white').generate_from_frequencies(keywords)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 3: Clean and Preprocess the Data\n",
    "\n",
    "We'll clean and preprocess the data by removing articles that are duplicates or empty.\n",
    "\n",
    "I considered removing unnecessary punctuation and filler words, but I decided against it because I think it would be better to leave the data as-is and let the model learn how to handle these words. For example, if we remove the word \"the\", then the model will not be able to learn that \"the\" is a filler word and should be ignored.\n",
    "\n",
    "If the results are not good enough, then we can try removing unnecessary punctuation and filler words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop_duplicates(subset=['Content', 'Summary'])\n",
    "df = df.dropna(axis=0, how='any')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 4: Tokenize the Text\n",
    "\n",
    "We'll tokenize the text by splitting the text into words and creating input sequences and output summaries. We'll also create a dictionary that maps each word to a unique integer. We'll use this dictionary to convert the words in the input sequences and output summaries to integers.\n",
    "\n",
    "We'll also create a reverse dictionary that maps each integer back to the original word. We'll use this reverse dictionary to convert the integers back to words.\n",
    "\n",
    "We'll also create a dictionary that maps each word to the number of times it appears in the dataset. We'll use this dictionary to filter out words that appear less than a certain number of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = WhitespaceTokenizer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "df['Summary_Tokens'] = df['Summary'].apply(lambda x: tk.tokenize(x))\n",
    "df['Content_Tokens'] = df['Content'].apply(lambda x: tk.tokenize(x))\n",
    "\n",
    "def lemmatize_ser(ser):\n",
    "    for i_l, l in enumerate(ser):\n",
    "        for i_w, w in enumerate(l):\n",
    "            ser.iloc[i_l][i_w]= lemmatizer.lemmatize(w)\n",
    "            \n",
    "lemmatize_ser(df['Summary_Tokens'])\n",
    "lemmatize_ser(df['Content_Tokens'])\n",
    "\n",
    "# display lemmatized version of tokens\n",
    "df['Content_Tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned data to a new file\n",
    "df.to_csv('data/cleaned.csv', index=False)\n",
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "4286f1ee66e41fbf26cca8876906444877a06270d30416029ae0afaadf35139a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
