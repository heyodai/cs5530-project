{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 - Model Architecture Selection\n",
    "\n",
    "We have two goals for this phase:\n",
    "\n",
    "1. Choose a deep learning model that is suitable for the task of summarization.  \n",
    "2. Define the architecture of the model, including the number of layers, hidden units, and embedding dimensions.\n",
    "\n",
    "Note that this notebook does not have any code. It is just a place to record our thoughts and ideas. We will do a code-based investigation during Phase 3."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 1: Choose a Model\n",
    "\n",
    "We will most likely go with a transformer-based model, such as BERT, T5, GPT-J, or GPT-Neo. However, we could also look into an encoder-decoder model or a pointer-generator network. It may be best to analyze the pros and cons of each model and run a few experiments to see which one works best for our task.\n",
    "\n",
    "Here's a table of the models that we are considering:\n",
    "\n",
    "| Model                                                      | Type                    | Key Features                                                                                                              |\n",
    "|------------------------------------------------------------|-------------------------|---------------------------------------------------------------------------------------------------------------------------|\n",
    "| BERT                                                       | Transformer-based model | Pre-trained, fine-tunable, attention-based, bidirectional                                                                 |\n",
    "| T5                                                         | Transformer-based model | Pre-trained, fine-tunable, text-to-text transfer learning, flexible architecture                                          |\n",
    "| GPT-J                                                      | Transformer-based model | Pre-trained, large scale, high-quality generation                                                                         |\n",
    "| GPT-Neo                                                    | Transformer-based model | Pre-trained, smaller than GPT-J, high-quality generation                                                                  |\n",
    "| Sequence-to-Sequence with Attention (Seq2Seq)              | Encoder-Decoder model   | Attention-based, flexible architecture, proven success for text summarization                                             |\n",
    "| Transformer-Based Encoder-Decoder Models (BART, MASS, XLM) | Encoder-Decoder model   | Transformer-based, pre-trained, fine-tunable, bidirectional, flexible architecture, proven success for text summarization |\n",
    "| Pointer-Generator Network for Abstractive Summarization    | Pointer-Generator model | Hybrid approach (extraction and abstraction), proven success for text summarization                                       |\n",
    "| Reinforcement Learning-Based Pointer-Generator Network     | Pointer-Generator model | Trained using reinforcement learning, state-of-the-art performance, proven success for text summarization                 |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goal 2: Define the Architecture\n",
    "\n",
    "We will need to decide on the number of layers, hidden units, and embedding dimensions for our model. The number of layers and hidden units will depend on the size of our dataset and the level of performance we want to achieve. The embedding dimensions will depend on the size of our vocabulary and the level of performance we want to achieve.\n",
    "\n",
    "The ideal numbers will vary depending on the model we choose. For example, a transformer-based model like BERT will have a different architecture than an encoder-decoder model like Seq2Seq. However, here are some general guidelines:\n",
    "\n",
    "1. **Number of Layers** \n",
    "    - For transformer-based models like BERT and GPT, a typical architecture might include 12-24 layers. \n",
    "    - For encoder-decoder models like Seq2Seq, a typical architecture might include 2-4 layers.\n",
    "2. **Hidden Units** \n",
    "    - For transformer-based models, the number of hidden units per layer can range from 64 to several thousand. \n",
    "    - For encoder-decoder models, a typical architecture might include 128-512 hidden units per layer.\n",
    "3. **Embedding Dimensions** \n",
    "    - For transformer-based models, the embedding dimensions can range from 128 to several thousand. \n",
    "    - For encoder-decoder models, a typical architecture might include 128-512 embedding dimensions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4286f1ee66e41fbf26cca8876906444877a06270d30416029ae0afaadf35139a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
